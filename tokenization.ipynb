{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYnEj1mtpMdYuuJvRSBOEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ismoil27/jaydariGPT/blob/main/tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tg_bTtqtz3a",
        "outputId": "da283953-6e21-4290-8f5d-bc9b8dbbf1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n"
          ]
        }
      ],
      "source": [
        "print('Hello world!') # token\n",
        "# 1 word => 1 token\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hello world this is a simple example to show how tokenization works in NLP and a sentence' # 17 token\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGdzW7qmt3ZY",
        "outputId": "3f8fa6c3-857e-4bf2-8863-5fcbf3840630"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world this is a simple example to show how tokenization works in NLP and a sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LylACEklulil",
        "outputId": "d9f3506d-3573-4aa6-b942-6c049becff9e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', 'this', 'is', 'a', 'simple', 'example', 'to', 'show', 'how', 'tokenization', 'works', 'in', 'NLP', 'and', 'a', 'sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "current_id = 0\n",
        "\n",
        "for token in tokens:\n",
        "  if token not in vocab:\n",
        "    vocab[token] = current_id\n",
        "    current_id +=1\n",
        "\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfcphfUnu7ee",
        "outputId": "b240a76d-a516-465c-90cc-b1045f688287"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Hello': 0, 'world': 1, 'this': 2, 'is': 3, 'a': 4, 'simple': 5, 'example': 6, 'to': 7, 'show': 8, 'how': 9, 'tokenization': 10, 'works': 11, 'in': 12, 'NLP': 13, 'and': 14, 'sentence': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = [vocab[token] for token in tokens]\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hMNYzncwNdD",
        "outputId": "b94dbc28-b584-42d8-8463-9bb111a4f1ae"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 4, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_vocab = {id: token for token, id in vocab.items()}\n",
        "decoded = \" \".join([reverse_vocab[id] for id in encoded])\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oAZsjoywxRM",
        "outputId": "ed4fb204-98d0-4542-9466-90d3e2b4e369"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world this is a simple example to show how tokenization works in NLP and a sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding vector => dense numerical array | Vector Database\n",
        "# 0 => [0.41, 0.22, -0.77, 0.56.........]\n",
        "# 1 => [0.34, 0.72, 0.77, 0.82.........]"
      ],
      "metadata": {
        "id": "pa5_-fLrxL0e"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 8\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "input_ids = torch.tensor([encoded])\n",
        "embeddings = embedding_layer(input_ids)\n",
        "\n",
        "\n",
        "print(embeddings)\n",
        "print(embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qq10qX_yPdL",
        "outputId": "a857fed7-a3ee-4183-f375-2b556c7b7325"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.1258,  0.2460,  1.3143, -1.2784, -1.4513,  0.7249, -0.4719,\n",
            "           0.6487],\n",
            "         [ 0.6209,  1.4133,  1.0931,  0.2682, -0.0119,  2.3800, -0.8791,\n",
            "           1.9438],\n",
            "         [ 0.1230, -0.1511,  0.7573,  1.1208,  0.1305, -1.7984,  0.9725,\n",
            "          -0.3203],\n",
            "         [-0.8356,  0.8769, -0.4962, -0.9548, -1.9509,  0.9479, -1.5003,\n",
            "          -0.8702],\n",
            "         [-1.4420,  1.4476,  1.5378,  1.1561,  1.4788,  1.6431,  0.6851,\n",
            "          -1.2843],\n",
            "         [-0.2397, -0.9280, -0.8575, -0.4059, -0.8115, -1.1991, -1.0120,\n",
            "          -0.1601],\n",
            "         [-0.3742,  0.7698, -0.6001,  0.9918, -0.8302, -0.7771, -1.0741,\n",
            "           0.1270],\n",
            "         [-0.5015, -0.7514, -0.9778,  1.1783, -0.3227,  0.5824,  0.5978,\n",
            "          -0.3365],\n",
            "         [ 0.7993,  1.5874,  0.1401, -0.0121, -0.6334, -0.0721,  0.4345,\n",
            "           0.6102],\n",
            "         [-0.6092,  0.5344, -2.0814, -1.1128,  0.5209,  0.4883,  1.1735,\n",
            "          -1.2967],\n",
            "         [ 1.7202, -1.4463,  1.6445, -2.3525, -0.5788, -1.7072, -0.5739,\n",
            "           0.7356],\n",
            "         [-0.4087, -1.4517, -1.2192, -1.9126, -0.6152,  1.9496,  0.0372,\n",
            "           1.9403],\n",
            "         [-0.8603,  1.7467, -0.8590, -2.1482, -1.0528,  0.7453, -1.2704,\n",
            "          -0.1264],\n",
            "         [-0.0556,  0.7873,  0.6833, -1.2036,  0.7722, -0.6428, -2.1486,\n",
            "          -0.4090],\n",
            "         [-0.3035,  1.1382,  0.4028,  1.5093, -0.7513,  0.5627,  1.9938,\n",
            "          -1.4966],\n",
            "         [-1.4420,  1.4476,  1.5378,  1.1561,  1.4788,  1.6431,  0.6851,\n",
            "          -1.2843],\n",
            "         [ 2.2106,  0.2323, -2.2363, -0.2701,  0.1382, -0.2092,  0.3169,\n",
            "          -0.8219]]], grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([1, 17, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyMBXMKeeJ9R",
        "outputId": "796cb760-acaa-430f-fe76-c873ae474dbf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "e5moEo7bo3Gc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Vocab size:', tokenizer.vocab_size)\n",
        "print('Special tokens:', tokenizer.special_tokens_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdbBMo93pdg-",
        "outputId": "2dbf8f1b-275d-41bc-df70-e3070e058dc8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 32000\n",
            "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello how are you doing?\"\n",
        "print('text:', text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw898Kz7ShT1",
        "outputId": "365f51e2-248f-45ac-a140-7c4db06b8169"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: Hello how are you doing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "print('tokens:', tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-fy0DO8SnWF",
        "outputId": "198f7771-7da2-42de-d68d-8287749acf4f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: ['▁Hello', '▁how', '▁are', '▁you', '▁doing', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print('token IDs:', ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFJn6UYqS1SD",
        "outputId": "8ef1f878-3026-49e3-eb58-f5391df18c18"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token IDs: [15043, 920, 526, 366, 2599, 29973]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer(text)\n",
        "print('encoded:',encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx3Igb9nTL1B",
        "outputId": "0ea5dc35-67ed-4d78-8996-c1b5add106fb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: {'input_ids': [1, 15043, 920, 526, 366, 2599, 29973], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Input IDs:', encoded['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waVQFI1FTbug",
        "outputId": "ef0c70df-60d2-44ef-e312-76789325c5e7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: [1, 15043, 920, 526, 366, 2599, 29973]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"Hi\" #=> [1, 345, 2, 2, 2] // length 5\n",
        "\"How are you?\" #=> [1, 45, 657, 234, 23453] // length 5\n",
        "# 'attention_mask': [1, 1, 0, 0, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hnh2ZNxr_7Pi",
        "outputId": "c6d3a426-113b-47d4-e609-9a8d3ababb03"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How are you?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(encoded[\"input_ids\"])\n",
        "print('decoded_text:', decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RlC1N1ABbv4",
        "outputId": "9bfc4e21-57b2-4714-dab3-bcc1c2cfd57a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoded_text: <s> Hello how are you doing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"Today we are learning about tokenizers.\",\n",
        "    \"TinyLlama is a small but powerful language model.\"\n",
        "]\n",
        "\n",
        "batch = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "print(batch.keys())\n",
        "\n",
        "print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
        "print('Attention mask shape:', batch[\"attention_mask\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG0n2vxgBlIY",
        "outputId": "9e0a4e7e-6b67-4ddd-a980-4dd466eec9e5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KeysView({'input_ids': tensor([[    1, 15043, 29892,   920,   526,   366, 29973,     2,     2,     2,\n",
            "             2,     2,     2,     2],\n",
            "        [    1, 20628,   591,   526,  6509,  1048,  5993, 19427, 29889,     2,\n",
            "             2,     2,     2,     2],\n",
            "        [    1,   323,  4901, 29931, 29880,  3304,   338,   263,  2319,   541,\n",
            "         13988,  4086,  1904, 29889]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])})\n",
            "Input IDs shape: torch.Size([3, 14])\n",
            "Attention mask shape: torch.Size([3, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [    1, 15043, 29892,   920,   526,   366, 29973,     2,     2,     2, 2,     2,     2,     2]\n",
        "# [    1, 20628,   591,   526,  6509,  1048,  5993, 19427, 29889,     2, 2,     2,     2,     2]\n",
        "# [    1,   323,  4901, 29931, 29880,  3304,   338,   263,  2319,   541, 13988,  4086,  1904, 29889]"
      ],
      "metadata": {
        "id": "XBc2GHDHBvQH"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('special tokens map:', tokenizer.special_tokens_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv39SHuNCeWK",
        "outputId": "f1798b71-0b39-42c5-f871-501a473e5f8c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "special tokens map: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"headerandshoulder\" # sub word\n",
        "print(\"word:\", word)\n",
        "print(\"Tokens:\", tokenizer.tokenize(word))\n",
        "print(\"IDs:\", tokenizer.encode(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRpYgNJjCmBv",
        "outputId": "5e3b1213-c26e-400c-dbb7-ee620a914bf9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: headerandshoulder\n",
            "Tokens: ['▁header', 'and', 'should', 'er']\n",
            "IDs: [1, 4839, 392, 9344, 261]\n"
          ]
        }
      ]
    }
  ]
}